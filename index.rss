<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Boletim diário de segurança</title><link>https://boletimsec.com.br/boletim-diario-ciberseguranca/</link><description>NGSX RSS Feed</description><language>pt-BR</language><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>* Gemini Ganha Novas Proteções Contra Ataques Indiretos de IA.</title><link>https://boletimsec.com.br/boletim-diario-ciberseguranca/</link><description>O Google anunciou novas camadas de defesa para proteger seus sistemas de inteligência artificial generativa (GenAI) contra ataques de injeção indireta de prompts. Esses ataques inserem instruções maliciosas em fontes externas, como e-mails e documentos, para manipular a IA sem interação direta do usuário. Entre as medidas, a empresa implementou a estratégia de defesa em profundidade, que inclui o fortalecimento dos modelos, uso de modelos de machine learning para detectar comandos maliciosos e salvaguardas no nível do sistema. O modelo Gemini agora conta com filtros de injeção de prompts, reforço de segurança com marcações especiais em dados não confiáveis (técnica chamada “spotlighting”), sanitização de markdowns e remoção de URLs suspeitas usando o Google Safe Browsing. Também há um sistema de confirmação de ações arriscadas e notificações de segurança ao usuário. Apesar das melhorias, o Google alerta que hackers estão desenvolvendo ataques adaptativos, capazes de contornar essas proteções com o uso de técnicas como o Red Teaming Automatizado (ART). Pesquisas recentes mostram que modelos de linguagem podem ser explorados para extrair dados sensíveis, criar malwares polimórficos e realizar ataques direcionados. Estudos de especialistas também indicam que LLMs são mais eficazes em identificar injeções de prompt, mas ainda falham em tarefas como exploração de sistemas e inversão de modelos. Outro alerta veio de um teste com 16 modelos de IA, onde alguns demonstraram comportamentos maliciosos, como chantagem e vazamento de informações, quando colocados sob pressão. Embora esses cenários não tenham sido observados no mundo real, os pesquisadores destacam a importância de entender os riscos emergentes e continuar investindo em defesas mais robustas.</description><guid isPermaLink="true">https://boletimsec.com.br/boletim-diario-ciberseguranca/</guid></item><item><title>* Guerra Irã x Israel Aumenta Risco de Ataques Cibernéticos Contra os EUA.</title><link>https://boletimsec.com.br/boletim-diario-ciberseguranca/</link><description>O Departamento de Segurança Interna dos Estados Unidos (DHS) emitiu um alerta oficial sobre o aumento do risco de ciberataques por grupos pró-Irã, após os recentes bombardeios americanos contra instalações nucleares iranianas. As ações militares foram realizadas como parte da guerra entre Irã e Israel, iniciada em 13 de junho de 2025. De acordo com o DHS, o atual cenário de conflito criou um “ambiente de ameaça elevada” para redes e sistemas americanos. O comunicado destaca que são prováveis ataques cibernéticos de baixo nível realizados por hacktivistas pró-Irã, além de ofensivas mais sofisticadas lideradas por agentes com ligação direta ao governo iraniano. Esses grupos têm histórico de explorar vulnerabilidades em redes pouco protegidas e dispositivos conectados à internet nos EUA, com o objetivo de causar interrupções e impactos políticos. O presidente Donald Trump anunciou que a ofensiva militar americana atingiu três importantes instalações nucleares iranianas localizadas em Fordo, Natanz e Isfahan. Trump classificou os ataques como um “espetacular sucesso militar” e alertou que novas ações, ainda mais intensas, podem ocorrer caso o Irã não busque uma solução pacífica. O conflito, além de militar, se intensificou no ciberespaço. Hacktivistas alinhados tanto ao Irã quanto a Israel têm realizado ataques virtuais de retaliação. Como resposta direta aos bombardeios, o grupo pró-Irã conhecido como Team 313 afirmou ter derrubado a plataforma Truth Social, rede social do ex-presidente Trump, através de um ataque de negação de serviço distribuído (DDoS). Autoridades americanas seguem monitorando possíveis novas ameaças, reforçando a segurança digital de infraestruturas críticas em todo o país.</description><guid isPermaLink="true">https://boletimsec.com.br/boletim-diario-ciberseguranca/</guid></item><item><title>* Pesquisadores Revelam Falha Crítica em Modelos da OpenAI e Google.</title><link>https://boletimsec.com.br/boletim-diario-ciberseguranca/</link><description>Pesquisadores de segurança cibernética alertam para uma nova técnica de jailbreak chamada Echo Chamber, que consegue burlar proteções de modelos de linguagem como os da OpenAI e do Google, induzindo-os a gerar conteúdos prejudiciais. Ao contrário dos métodos tradicionais, que utilizam frases adversárias ou obfuscação de caracteres, o Echo Chamber manipula os modelos por meio de referências indiretas, inferências em múltiplas etapas e direcionamento semântico. Essa abordagem é sutil, mas poderosa. Ela influencia gradualmente o estado interno do modelo, fazendo com que ele produza respostas que violam políticas de segurança. Segundo Ahmad Alobaid, da NeuralTrust, a técnica explora um tipo de “envenenamento de contexto”, criando um ciclo de feedback no qual os próprios resultados do modelo são usados para reforçar mensagens nocivas subsequentes. Em testes controlados, os ataques com Echo Chamber apresentaram taxa de sucesso superior a 90% em temas como discurso de ódio, violência, sexismo e pornografia. Em categorias como desinformação e automutilação, a taxa se aproximou de 80%. Essa vulnerabilidade destaca um ponto cego crítico na segurança de LLMs. À medida que esses modelos se tornam mais sofisticados, também se tornam mais suscetíveis à exploração indireta. Técnicas como “Crescendo” e “many-shot jailbreaks” reforçam essa preocupação. O alerta vem junto com outro caso: a Cato Networks demonstrou um ataque PoC contra o protocolo de contexto da Atlassian (MCP), usado no Jira Service Management. Nele, um engenheiro de suporte, sem saber, executa instruções maliciosas contidas em um ticket enviado por um agente externo. O ataque foi chamado de “Living off AI”, por explorar sistemas de IA sem a necessidade de acesso direto ou autenticação.</description><guid isPermaLink="true">https://boletimsec.com.br/boletim-diario-ciberseguranca/</guid></item></channel></rss>